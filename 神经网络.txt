神经网络构造：
输出层
隐藏层
输出层

神经网络实质上是一个拟合的过程
其中有一个函数relu（）
从输出到隐藏是对relu的一次线性变换
在隐藏层relu本身会对数据进行非线性变换
从隐藏到输出，又会对relu进行一次线性变换，从而达到拟合的效果


线性变换中wx+b
w称之为权重
b称之为偏置bias

神经网络所用的地方大多都是非线性的拟合
而输入大多为线性，所以需要一个非线性的函数，使线性变为非线性。这种使输入变的有效的函数，称之为激活函数，例如relu

线性激活函数也有 例如linear 但这种只能去处理线性拟合


同时，拟合出来的函数，与实际的函数不可能完全拟合，会存在损失loss
所以会有损失函数
一般用的损失函数都是均方差

通过不断调整w和b，使得损失函数得出的损失loss最小，从而达到最好的拟合。



激活函数：
sigmoid
relu
Tanh
Leaky ReLU
