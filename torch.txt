#深度学习
1.Linear Regression 线性回归
y=wx+b
通过求出loss=[（wx+b）-y]²的最小值
找出最符合和w与b  从而形成函数

与之相反的是“logistic Regression” 逻辑回归 将数据压缩到[0,1]

2.成本函数
J(w,b)=1/2m * 累计求和[（w xi + b）- yi ]²
这个求出来的是平均平方误差 为了是不让平方误差和随着数据的增多而增大 将它控制在一个小值
其中m表示样本点个数
除以2是让数据更简洁

在做成本函数可视化时：
作图时 要先将b和w分别作为变量  即Y轴为误差J x轴为w或者b
第二步结合w和b产生一幅三维图像

如何通过程序求得J最小的w和b？ 就要通过梯度下降算法

3.梯度下降算法
成本函数J(w,b)
原理：
w1=w1-lr*J‘(w1) //lr表示learning rate
b1=b1-lr*J‘(b1) //上一个式子将w作为变量 这个式子是将b作为变量 但式子是同一个
需要注意的是 两个变量需要同时更新 即：要计算完两个变量之后才可以更新new_w new_b
否则就不是同一个式子了 计算会错误

J‘(w1)和J‘(b1)就是w和b在这组数据下的梯度值

目的：找到局部最小量 而非全局最小量

3.1梯度
所谓梯度就是一个方向向量


4.线性回归中低梯度下降算法
将成本函数带入到梯度下降算法 再进行求导可得
w=w-lr*1/m * 累计求和[(f(xi)-yi)*xi]
b=b-lr*1/m * 累计求和(f(xi)-yi)
xi表示每个所给数据点的横坐标
每个不同的w或者b 都要计算一遍所有数据点 然后累计求和
对于b变量来说 它的累计求和不用乘xi

5.tensor 张量
结构和np.array()相类似
#创建tensor
a=torch.tensor([1,2,3])


#创建全是1的tensor
a=torch.ones(2,3)#两行三列
tensor([[1,1,1]
        [1,1,1]])
#
a=torch.zeros(2,3)
#创建均匀分布的随机张量 数值在[0,1)

a=torch.rand(2,3)
#创建标准正态分布的随机张量 均值为0 方差为1
a=torch.randn(2,3)

a.dtype#tensor的数据类型
a.shape #shape是一个属性/变量
a.size() #size是一个方法

#numpy转tensor
a=np.ones(5)
b=torch.from_numpy(a)

#
b=torch.ones(2,3)
a=b.numpy()

需要注意的是 tensor和array相互转换后 是共享一块内存的 当其中一个的值改变时 另一个也会发生改变

#tensor的索引
和array一模一样
a=torch.tensor([1,2,3],[4,6,7])
a[0,0]
a[1,2]

6.tensor.size()
torch.Size([2, 3, 3])
有几个数字代表它是几维
这个代表3维
2层 3*3的tensor

torch.Size([10])
1维
1层 1*5的tensor

7.前向传播和反向传播
前：将信号传入模型 经过计算得到输出
反向：从输出层往回传 用于训练过程中传输误差 优化结果

8.激活函数
线性函数无论通过多少次的线性变换都只能是线性
所以如何表示非线性函数呢？

这时我们就需要将线性函数乘以非线性函数 这个过程所用的非线性函数也称为激活函数

激活函数的例子：
一：sigmoid函数
缺点：不易收敛
     会产生梯度消失的现象 （即求导之后在x很大或很小的时候 导数值为0 导致梯度消失）
二：tanh函数
缺点：梯度消失
三：ReLU函数
可以很好解决上面的两个缺点
同时可以动态控制神经元状态 （小于等于0会被抑制 大于0激活）称之为稀疏性

缺点：会导致一些神经元始终无法激活 导致神经元坏死
     因为ReLU函数没有上限 所以可能导致梯度数值过大 导致梯度爆炸
但这些缺点可以被 优化的ReLU函数所解决

9.机器学习的流程
一：数据获取
二：特征工程
三：建立模型
四：应用

深度学习 就在特征工程中提取特征的算法

10.神经网络
一：得分
对于不同像素点的分析 为了识别出具体是什么东西 每个像素点的权重weights是不同的

对于像素的处理 实质上是矩阵相乘[n*m]*[m*l] 得到[n*l]
前一个是权重矩阵 后一个是图像像素矩阵
对于识别不同的物体 分析时所用的权重是不同的

通过矩阵相乘 我们可以得到对于这张图片 它属于每一个类别的得分是多少
比如对于猫这个类别 这个图片得到200分
对于狗这个类别 图片得到300分
通过分数的高低 判断这张图片是什么东西

二：损失函数
根据算出来的得分 计算损失
但这个损失函数 不是成本函数
成本函数是在反向传播中使用的

三：正则化
正则化惩罚项
是不是两个函数的 数据损失data_loss 一样 两个函数就一样呢？
x=[1,1]
w1=[1,0]
w2=[0.5,0.5]
这两个data_loss一样 但函数不一样
所以在损失函数data_loss算出来之后 要加上正则化惩罚项

这个惩罚项由权重w所决定  是 ∑wi²*惩罚系数k
去除一些过拟合数据 即数值波动很大的函数

四：softmax分类器/sigmoid函数
对分数进行映射后再归一化 得到这张图片是某个物体的概率
第二步对第一步得到的值进行log处理 得到属于[0，1]的 真实损失值

五：由一二三 得分到求得损失值 是神经网络的前向传播过程
神经网络分为前向传播和反向传播

11.反向传播






