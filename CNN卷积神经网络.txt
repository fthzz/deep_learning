整体架构分为：
输入层
卷积层CONV
池化层
全连接层
 
因为cnn处理的是多维数据，rgb三个通道都需要处理，所以数据光用数组肯定不行，所以有了三维的张量tensor

提取特征需要用 权重参数矩阵，也就是一个核ker，在cnn中称为卷积核

例1：比如说这个卷积核是4*4*3。也就是说在一个通道下，一个4*4的特征矩阵w，从一个4*4的像素区域提取出一个特征值。
注：上面这个例子中的卷积核有三层，但每一层的权重参数矩阵不一定都是一样的
注：特征值计算都是内积，即对应位置相乘相加，不是矩阵相乘

例2：每个权重参数除了卷积核，还有一个 偏置bias。在计算特征值时，是将三个通道的特征值相加，再加上偏置bias。


计算过后，由特征值组成的图，我们称之为特征图，activation maps。我们通常通过对一张图片用多个卷积核（包含多个偏置bias），获得多个特征图，然后堆叠在一起，获得丰富的特征图。
注：在使用多个卷积核的时候，要保证卷积核的结构是一样的，只有数值不同

注：卷积操作一次是不够的，在得到一个多维特征图后，再对这个特征图进行卷积。直到提取到合理的特征。




卷积层参数：
1滑动窗口步长/stride：卷积核在图层上每次移动的格数
2卷积核尺寸
3卷积核个数
4边缘填充padding

例3：通过卷积核去遍历，会出现一些像素点被重复多次利用，而边缘像素点只被用到一次，导致忽略边缘特征的问题。所以出现边缘填充，图像外围围一圈全为0的像素点，使边缘像素也被多次计算。



池化层：
当用的卷积核过多，一次卷积可能会产出256*256*100的特征图，数据量过大，池化层做的就是下采样，简称压缩特征

压缩方法：
最大池化/max pooling：去一块区域中最大的那个特征值，作为压缩后的特征值




cnn整体流程：
卷积层CONV->非线性变换RELU/激活层->池化POOL->全连接层FC
这两个是一起使用
两次卷积 一次池化重复
relu本身是一个激活函数




经典网络架构：vgg，resnet何凯明














